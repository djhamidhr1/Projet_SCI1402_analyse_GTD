# SCI 1402 - Projet en science des données
# Rapport de projet

**Votre prénom et nom :** HAMID DIGORGORD  
**Votre numéro d'étudiant :** 23110186  
**Trimestre d'inscription :** Automne 2024  
**Date d'envoi :** 18 décembre 2024

## Présentation générale du projet

**Nom du projet sélectionné**  
Analyse de la base de données mondiale sur le terrorisme (GTD)

**Motivation lors de la sélection du projet**  
Ce projet a été choisi pour sa pertinence sociétale et son potentiel d'analyse approfondie. La GTD représente la plus grande base de données sur le terrorisme mondial, offrant une opportunité unique d'appliquer des techniques avancées de science des données pour comprendre les tendances et patterns des activités terroristes.

**Définition du projet**  
Le projet consiste à analyser la base de données GTD couvrant la période 1970-2017, comprenant plus de 180,000 incidents terroristes. L'objectif est d'explorer les tendances historiques, identifier les facteurs d'influence, et développer des modèles prédictifs pour mieux comprendre et anticiper les incidents terroristes.

**Rappel des objectifs visés**
1. Analyser les distributions et tendances des attaques terroristes
2. Identifier les patterns géographiques et temporels
3. Développer des modèles prédictifs
4. Créer un tableau de bord interactif
5. Produire des analyses statistiques détaillées

**Planification du projet**  
Tâches et échéancier :
1. Configuration (Semaines 1-2)
   - Installation des outils
   - Configuration de l'environnement
   
2. Exploration des données (Semaines 3-5)
   - Import et nettoyage
   - Analyses préliminaires
   
3. Analyse approfondie (Semaines 6-9)
   - Analyses statistiques
   - Visualisations
   
4. Modélisation (Semaines 10-12)
   - Développement des modèles
   - Validation
   
5. Finalisation (Semaines 13-15)
   - Création du dashboard
   - Documentation

**Risques identifiés :**
- Volume important de données
- Complexité des analyses
- Contraintes temporelles

**Évaluation de la performance**
Critères établis :
1. Qualité du nettoyage des données
2. Pertinence des analyses
3. Performance des modèles
4. Utilisabilité du dashboard
5. Respect des délais

## Explication des écarts de la réalisation

**Résultats atteints**
1. Configuration réussie de l'environnement
2. Analyse de 181,593 incidents
3. Développement de visualisations interactives
4. Identification des patterns clés

**Justification des choix**

*Approches et méthodes :*
- Utilisation de R et RStudio pour l'analyse
- Apache Spark pour le traitement des données volumineuses
- Approche itérative pour le développement

*Techniques appliquées :*
- Analyse exploratoire des données
- Visualisation géospatiale
- Modélisation statistique
- Analyse temporelle

*Outils utilisés :*
- R/RStudio pour l'analyse
- Git pour le versioning
- Spark pour le Big Data
- Shiny pour les visualisations

**Problèmes rencontrés**
1. Volume de données important → Résolu avec Spark
2. Données manquantes → Stratégies de nettoyage adaptées
3. Complexité des visualisations → Optimisation progressive

## Conclusion et recommandations

**Ce que je ferais différemment**
1. Commencer plus tôt l'optimisation des performances
2. Mettre en place une structure de données plus efficace
3. Automatiser davantage les processus de nettoyage

**Apprentissage le plus significatif**  
La gestion efficace de grandes quantités de données et l'importance d'une approche structurée dans l'analyse de données complexes.

## Annexes

**Journal des activités**

*Semaines 1-2 (30h)*
- Installation et configuration de l'environnement
- Familiarisation avec les données

*Semaines 3-5 (45h)*
- Import et nettoyage des données
- Analyses préliminaires

*Semaines 6-9 (60h)*
- Développement des visualisations
- Analyses statistiques approfondies

*Semaines 10-12 (40h)*
- Modélisation
- Tests et validation

*Semaines 13-15 (35h)*
- Création du dashboard
- Documentation et rapport final
