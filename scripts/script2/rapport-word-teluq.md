[Logo TÉLUQ]

# SCI 1402
# Projet en science des données
# Rapport de projet
(4 pages maximum)

**Votre prénom et nom :** HAMID DIGORGORD
**Votre numéro d'étudiant :** 23110186
**Trimestre d'inscription :** Automne 2024
**Date d'envoi :** 18 décembre 2024

## Présentation générale du projet

### Nom du projet sélectionné
Analyse de la base de données mondiale sur le terrorisme (GTD)

### Motivation lors de la sélection du projet
J'ai choisi ce projet pour sa pertinence sociétale et son potentiel d'analyse approfondie. La GTD représente la plus grande base de données sur le terrorisme mondial, offrant une opportunité unique d'appliquer des techniques avancées de science des données pour comprendre les tendances et patterns des activités terroristes. Ce choix permet d'exploiter pleinement les compétences acquises durant ma formation tout en contribuant à la compréhension d'un phénomène mondial important.

### Définition du projet
Le projet consiste à analyser la base de données GTD couvrant la période 1970-2017, comprenant plus de 180,000 incidents terroristes. L'analyse inclut :
- Traitement et nettoyage des données brutes
- Analyse statistique approfondie
- Développement de visualisations interactives
- Création de modèles prédictifs
- Élaboration d'un tableau de bord dynamique

### Rappel des objectifs visés lors de la sélection du projet
1. **Analyse exploratoire des données (EDA)**
   - Comprendre la distribution des données
   - Identifier les tendances temporelles et géographiques
   - Explorer les corrélations entre variables

2. **Visualisation des données**
   - Créer des visualisations interactives
   - Développer des cartes géographiques
   - Illustrer les tendances temporelles

3. **Modélisation prédictive**
   - Identifier les facteurs d'influence
   - Développer des modèles de prédiction
   - Évaluer la performance des modèles

### Planification du projet
**Structure de découpage du projet :**

1. **Phase initiale (Semaines 1-2)**
   - Configuration de l'environnement
   - Installation des outils nécessaires
   - Importation des données

2. **Phase d'exploration (Semaines 3-5)**
   - Nettoyage des données
   - Analyses préliminaires
   - Validation des données

3. **Phase d'analyse (Semaines 6-9)**
   - Analyses statistiques approfondies
   - Développement des visualisations
   - Identification des patterns

4. **Phase de modélisation (Semaines 10-12)**
   - Construction des modèles
   - Tests et validation
   - Optimisation des performances

5. **Phase finale (Semaines 13-15)**
   - Création du dashboard
   - Documentation
   - Finalisation du rapport

### Évaluation de la performance de la réalisation
**Critères quantitatifs :**
- Nombre d'incidents analysés : 181,593
- Couverture temporelle : 47 ans
- Précision des modèles prédictifs : >85%

**Critères qualitatifs :**
- Qualité du nettoyage des données
- Pertinence des visualisations
- Utilisabilité du dashboard

## Explication des écarts de la réalisation par rapport à la planification

### Rappel des résultats réellement atteints
- Configuration complète de l'environnement technique
- Analyse de 181,593 incidents terroristes
- Développement de visualisations interactives
- Identification des patterns temporels et géographiques
- Création d'un dashboard fonctionnel

### Justification des choix faits en cours de réalisation

#### Approche ou méthodes
- Méthodologie itérative pour le développement
- Approche data-driven pour les analyses
- Validation continue des résultats

#### Techniques appliquées
- Nettoyage avancé des données
- Analyse statistique descriptive et inférentielle
- Modélisation prédictive
- Visualisation géospatiale

#### Outils utilisés
- R et RStudio pour l'analyse
- Git pour le versioning
- Apache Spark pour le traitement des données
- Shiny pour le dashboard interactif

### Énoncé des problèmes rencontrés
1. Volume important de données → Résolu avec Apache Spark
2. Données manquantes → Stratégies de nettoyage développées
3. Performance des visualisations → Optimisation progressive

## Conclusion et recommandations

### Apprentissage réalisé

#### Ce que vous feriez différemment si c'était à refaire
1. Structurer plus efficacement la base de données dès le début
2. Automatiser davantage les processus de nettoyage
3. Implémenter plus tôt les tests de validation

#### Décrire l'apprentissage le plus significatif
L'apprentissage le plus significatif a été la maîtrise de la gestion et de l'analyse de données massives, combinant des compétences techniques en programmation avec une compréhension approfondie des méthodes statistiques.

## Annexe(s)

### Journal des activités

**Semaines 1-2 (30 heures)**
- Installation environnement R/RStudio
- Configuration Git et Spark
- Import initial des données

**Semaines 3-5 (45 heures)**
- Nettoyage des données
- Analyses exploratoires
- Documentation préliminaire

**Semaines 6-9 (60 heures)**
- Développement des visualisations
- Analyses statistiques
- Tests de performance

**Semaines 10-12 (40 heures)**
- Modélisation prédictive
- Validation des modèles
- Optimisation

**Semaines 13-15 (35 heures)**
- Développement du dashboard
- Documentation finale
- Rédaction du rapport

Total : 210 heures

